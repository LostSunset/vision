{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Transforms on Rotated Bounding Boxes\n\nThis example illustrates how to define and use rotated bounding boxes. We'll\ncover how to define them, demonstrate their usage with some of the existing\ntransforms, and finally some of their unique behavior in comparision to\nstandard bounding boxes.\n\nFirst, a bit of setup code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2\nfrom helpers import plot\n\nplt.rcParams[\"figure.figsize\"] = [10, 5]\nplt.rcParams[\"savefig.bbox\"] = \"tight\"\n\n# if you change the seed, make sure that the randomly-applied transforms\n# properly show that the image can be both transformed and *not* transformed!\ntorch.manual_seed(0)\n\n# If you're trying to run that on Colab, you can download the assets and the\n# helpers from https://github.com/pytorch/vision/tree/main/gallery/\norig_img = Image.open(Path('../assets') / 'leaning_tower.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Rotated Bounding Box\nRotated bounding boxes are created by instantiating the\n:class:`~torchvision.tv_tensors.BoundingBoxes` class. It's the `format`\nparameter of the constructor that determines if a bounding box is rotated or\nnot. In this instance, we use the\n:attr:`~torchvision.tv_tensors.BoundingBoxFormat` kind `CXCYWHR`. The first\ntwo values are the `x` and `y` coordinates of the center of the bounding box.\nThe next two values are the `width` and `height` of the bounding box, and the\nlast value is the `rotation` of the bounding box.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "orig_box = tv_tensors.BoundingBoxes(\n    [\n        [860.0, 1100, 570, 1840, -7],\n    ],\n    format=\"CXCYWHR\",\n    canvas_size=(orig_img.size[1], orig_img.size[0]),\n)\n\nplot([(orig_img, orig_box)], bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rotation\nRotated bounding boxes maintain their rotation with respect to the image even\nwhen the image itself is rotated through the\n:class:`~torchvision.transforms.RandomRotation` transform.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rotater = v2.RandomRotation(degrees=(0, 180), expand=True)\nrotated_imgs = [rotater((orig_img, orig_box)) for _ in range(4)]\nplot([(orig_img, orig_box)] + rotated_imgs, bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Padding\nRotated bounding boxes also maintain their properties when the image is padded using\n:class:`~torchvision.transforms.Pad`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "padded_imgs_and_boxes = [\n    v2.Pad(padding=padding)(orig_img, orig_box)\n    for padding in (30, 50, 100, 200)\n]\nplot([(orig_img, orig_box)] + padded_imgs_and_boxes, bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resizing\nRotated bounding boxes are also resized along with an image in the\n:class:`~torchvision.transforms.Resize` transform.\n\nNote that the bounding box looking bigger in the images with less pixels is\nan artifact, not reality. That is merely the rasterised representation of the\nbounding box's boundaries appearing bigger because we specify a fixed width of\nthat rasterized line. When the image is, say, only 30 pixels wide, a\nline that is 3 pixels wide is relatively large.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resized_imgs = [\n    v2.Resize(size=size)(orig_img, orig_box)\n    for size in (30, 50, 100, orig_img.size)\n]\nplot([(orig_img, orig_box)] + resized_imgs, bbox_width=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perspective\nThe rotated bounding box is also transformed along with the image when the\nperspective is transformed with :class:`~torchvision.transforms.RandomPerspective`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perspective_transformer = v2.RandomPerspective(distortion_scale=0.6, p=1.0)\nperspective_imgs = [perspective_transformer(orig_img, orig_box) for _ in range(4)]\nplot([(orig_img, orig_box)] + perspective_imgs, bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elastic Transform\nThe rotated bounding box is appropriately unchanged when going through the\n:class:`~torchvision.transforms.ElasticTransform`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "elastic_imgs = [\n    v2.ElasticTransform(alpha=alpha)(orig_img, orig_box)\n    for alpha in (100.0, 500.0, 1000.0, 2000.0)\n]\nplot([(orig_img, orig_box)] + elastic_imgs, bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crop & Clamping Modes\nThe :class:`~torchvision.transforms.CenterCrop` transform selectively crops\nthe image on a center location. The behavior of the rotated bounding box\ndepends on its `clamping_mode`. We can set the `clamping_mode` in the\n:class:`~torchvision.tv_tensors.BoundingBoxes` constructur, or by directly\nsetting it after construction as we do in the example below.\n\nThere are two values for `clamping_mode`:\n\n - `\"soft\"`: The default when constucting\n   :class:`~torchvision.tv_tensors.BoundingBoxes`. <Insert semantic\n   description for soft mode.>\n - `\"hard\"`: <Insert semantic description for hard mode.>\n\nFor standard bounding boxes, both modes behave the same. We also need to\ndocument:\n\n - `clamping_mode` for individual kernels.\n - `clamping_mode` in :class:`~torchvision.transforms.v2.ClampBoundingBoxes`.\n - the new :class:`~torchvision.transforms.v2.SetClampingMode` transform.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert orig_box.clamping_mode == \"soft\"\nhard_box = orig_box.clone()\nhard_box.clamping_mode = \"hard\"\n\nsoft_center_crops_and_boxes = [\n    v2.CenterCrop(size=size)(orig_img, orig_box)\n    for size in (800, 1200, 2000, orig_img.size)\n]\n\nhard_center_crops_and_boxes = [\n    v2.CenterCrop(size=size)(orig_img, hard_box)\n    for size in (800, 1200, 2000, orig_img.size)\n]\n\nplot([[(orig_img, orig_box)] + soft_center_crops_and_boxes,\n      [(orig_img, hard_box)] + hard_center_crops_and_boxes],\n     bbox_width=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}